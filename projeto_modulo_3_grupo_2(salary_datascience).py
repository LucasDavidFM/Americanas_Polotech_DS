# -*- coding: utf-8 -*-
"""projeto_modulo 3-grupo_2(salary_datascience)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u0XZDsCMmMpOUlEDtM3I-gWE7LpQF0cd

# Etapa de Verificação e Tratamento do dataset

**Este collab tem como propósito a correção de incongruências do dataset**

Anotações de problemas encontrados no datasets:
- Valores nulos encontrados nas tabelas: 
company
level
tag
gender
other details
Race
Education

- Repetição de valores encontrados na coluna "company",nomes das empresas contratantes se repetem em formas diferentes (ex: Microsoft e microsoft)

- Coluna "level" apresenta muitos resultados, acredito ser uma coluna problemática

- Em uma consulta a olho nu, percebi alguns outliers na coluna "totalyearlycompensation"

- Valores estranhos nas colunas "yearsofexperience" e "yearsatcompany", necessario estudo sobre o dataset( conferir kaggle)

- Coluna basesalary possui outliers e valores com 0

-

Correções do dataset:

Valores nulos
- company:
- level: Dropar
- tag: Dropar
- gender: alterar "Nan" para "Não informado" (tirar dúvida com professor para ver se é utilizável ou não)
- other details: Dropar
- Race: alterar "Nan" para "Não informado" (tirar dúvida com professor para ver se é utilizável ou não)
- Education: "Nan" para "Não informado" (tirar dúvida com professor para ver se é utilizável ou não)
-location: Dividir em 3 colunas (cidade, estado e páis). Depois dropar colunas não importantes

Tabelas que vão ser dropadas: (posteriormente explicar o motivo do drop)
- timestamp
- rownumber
- Masters_Degree
- Bachelors_Degree
- Doctorate_Degree
- Highschool
- Some_College
- Race_Asian
- Race_White
- Race_Two_Or_More
- Race_Black 
- Race_Hispanic   

Tabelas com correções especificas:
- Company: usar função str.title()
- basesalary: substituir os '0' pela média ou mediana ou moda dos valores sem outliers
- totalyearlycompensation: substituir os '0' pela média ou mediana ou moda dos valores sem outliers

Tabelas para análise:
- cityid
- dmaid
- yearsofexperience: Conferir os valores abaixo de 0 e ver o quantidade
- yearsatcompany: Conferir os valores abaixo de 0 e ver o quantidade

#Etapa de Verificação dos dados
"""

import pandas as pd
import numpy as np

df_salario = pd.read_csv("/content/Levels_Fyi_Salary_Data.csv", sep = ',')
df_salario.head()

df_salario.shape

df_salario.dtypes

"""Alteração para que consigamos visuzalizar no display do colab mais linhas e colunas"""

pd.set_option('display.max_columns', 30)

pd.set_option('display.max_rows', 80000)

df_salario.head()

"""Verificação do tamanho da base em questão com todas as linhas e colunas"""

df_salario.shape

"""Verificando quantos valores NA temos dentro da base em questão, foi um ponto crucial onde tivemos que decidir o que fariamos com algumas colunas, exemplo gender otherdetails e afins"""

df_salario.isnull().sum()

"""Foi verificado nas principais tabelas quantas valores diferentes tinhamos nos 62 mil registros"""

df_salario['company'].value_counts()

df_salario['level'].value_counts()

df_salario['title'].value_counts()

df_salario['totalyearlycompensation'].value_counts()

df_salario['location'].value_counts()

df_salario['yearsofexperience'].value_counts()

df_salario['yearsatcompany'].value_counts()

df_salario['tag'].value_counts()

df_salario['basesalary'].value_counts()

df_salario['Race'].value_counts()

df_salario['Education'].value_counts()

df_salario['gender'].value_counts()

df_salario['otherdetails'].value_counts()

df_salario['cityid'].unique()

"""Copiando o data set original para fazer alterações sem fazer a alteração na nossa fonte em questão"""

df_salario_teste = df_salario.copy()

df_salario_teste.isna().sum()

"""Verificação de quantos outliers tinhamos na coluna base de salario que junto com a coluna Ano salario são as principais do nosso data set"""

import numpy as np



def calcula_out(X):


  q1,q3 = np.quantile(X,[0.25,0.75])
  iqr =  q3 - q1
  is_outliers = ((X<(q1-1.5*iqr)) | (X>(q3+1.5*iqr)))
  outliers_count  = np.count_nonzero(is_outliers)

  return is_outliers,outliers_count, X[is_outliers]


is_outlier, outliers_count, outliers = calcula_out(df_salario_teste['basesalary'])
print(outliers_count)
#print(outliers)

import numpy as np



def calcula_out(X):


  q1,q3 = np.quantile(X,[0.25,0.75])
  iqr =  q3 - q1
  is_outliers = ((X<(q1-1.5*iqr)) | (X>(q3+1.5*iqr)))
  outliers_count  = np.count_nonzero(is_outliers)

  return is_outliers,outliers_count, X[is_outliers]


is_outlier, outliers_count, outliers = calcula_out(df_salario_teste['totalyearlycompensation'])
print(outliers_count)
#print(outliers)

"""Usamos a função .corr() que nos traz a matriz de correlação para termos uma ideia de quais colunas se relacionavam entre si"""

# Matriz de correlação
df_salario.corr()

df_salario_teste.shape

"""Após ponderações entre os menbros do grupo decidimos dropar as seguintes tabelas

Drops:

- timestamp
- rownumber
- Masters_Degree
- Bachelors_Degree
- Doctorate_Degree
- Highschool
- Some_College
- Race_Asian
- Race_White
- Race_Two_Or_More
- Race_Black 
- Race_Hispanic
- otherdetails
- level
- tag

feito o drop de todas as tabelas em questão
"""

# Drops de colunas
df_salario_teste.drop(['timestamp', 'rowNumber','Masters_Degree','Bachelors_Degree','Doctorate_Degree','Highschool','Some_College','Race_Asian','Race_White','Race_Two_Or_More','Race_Black','Race_Hispanic','otherdetails','level','tag'], axis=1, inplace = True)

"""Houve colunas que apesar de termos muitos valores nulos queriamos em nossa amostra para fazermos comparativos e verificações dessa forma resolvemos tratar os valores nulos substituindo pelo valor "Não Informado", houve tambem um caso onde o title acabou passando para a coluna gender fizemos a correção

Correção valores nulos:

- gender
- Race
- Education
"""

# Correção de valores nulos
df_salario_teste['gender'].fillna('Não informado', inplace = True)
df_salario_teste['Race'].fillna('Não informado', inplace = True)
df_salario_teste['Education'].fillna('Não informado', inplace = True)
df_salario_teste['company'].fillna('Não informado', inplace = True)
# Replace na coluna gender
df_salario_teste['gender'].replace('Title: Senior Software Engineer','Não informado', inplace = True)

"""Houve a necessidade de dividirmos a coluna locations em mais de uma para que tivessemos cidade, estado, pais.

Após muita ponderação sobre a questão dos valores 0 decidimos substituir pela media, mediana ou moda porem existiam os outliers que poderiam trazer uma ideia erroneas dos dados, dessa forma decidimos fazer esses calculos somente com os valores que não eram outliers 

correções especificas:
- location: Fazer split da coluna em 3, depois dropar as sobresalentes 
"""

#Com o split, forma criados 4 colunas. São 4 colunas por que um dos valores esta como "Korea, South"
df_salario_teste[['Col1','Col2', 'Col3' , 'Col4']] = df_salario_teste['location'].str.split(r',', expand = True)

"""
- totalyearlycompensation: substituir os '0' pela média ou mediana ou moda dos valores sem outliers"""

def calcula_out(X):


  q1,q3 = np.quantile(X,[0.25,0.75])
  iqr =  q3 - q1
  is_outliers = ((X<(q1-1.5*iqr)) | (X>(q3+1.5*iqr)))
  not_outliers = ((X>(q1-1.5*iqr)) & (X<(q3+1.5*iqr)))
  outliers_count  = np.count_nonzero(is_outliers)

  return is_outliers,outliers_count, X[is_outliers], X[not_outliers]


is_outlier, outliers_count, outliers, not_outliers = calcula_out(df_salario_teste['totalyearlycompensation'])
print(outliers_count)
print(outliers.count())

media_totalyearlycompensation = not_outliers.mean()
print(media_totalyearlycompensation)

"""- basesalary: substituir os '0' pela média ou mediana ou moda dos valores sem outliers"""

def calcula_out(X):


  q1,q3 = np.quantile(X,[0.25,0.75])
  iqr =  q3 - q1
  is_outliers = ((X<(q1-1.5*iqr)) | (X>(q3+1.5*iqr)))
  not_outliers = ((X>(q1-1.5*iqr)) & (X<(q3+1.5*iqr)))
  outliers_count  = np.count_nonzero(is_outliers)

  return is_outliers,outliers_count, X[is_outliers], X[not_outliers]


is_outlier, outliers_count, outliers, not_outliers = calcula_out(df_salario_teste['basesalary'])
print(outliers_count)
print(outliers.count())

media_basesalary = not_outliers.mean()
print(media_basesalary)

# Trocar valores 0 pela média sem os outliers
df_salario_teste['totalyearlycompensation'].replace(0,media_totalyearlycompensation, inplace = True)
df_salario_teste['basesalary'].replace(0,media_basesalary, inplace = True)

df_salario_teste['basesalary'].replace(0,media_basesalary, inplace = True)

df_salario_teste.to_csv('Levels_Fyi_Salary_Data_tratado.csv' , index = False)

df_salario_teste.dtypes

df_salario_teste.isnull().sum()

df_salario_teste['location'].unique()

df_salario_teste['location'].unique().sum()

df_salario_teste['Col3'].replace(' Korea','South Korea', inplace = True)

df_salario_teste.drop(['Col4'], axis=1, inplace = True)

df_salario_teste['Col3'].fillna('United States', inplace = True)

df_salario_teste.rename(columns ={"company": "compania",
                          "title": "profissao",
                          "totalyearlycompensation":"ganhoAnual",
                          "yearsatcompany":"temponaEmpresa",
                          "yearsofexperience":"anosExperiencia",
                          "basesalary":"salarioBase",
                          "stockgrantvalue":"SGV",
                          "gender":"genero",
                          "Race":"raca",
                          "Education":"educacao",
                          "Col1":"cidade",
                          "Col2":"estado",
                          "Col3":"pais",},
                inplace=True )

"""Verificação dos dados após tratamento"""

df_salario_teste.head()

df_salario_teste.isna().sum()

df_salario_teste.groupby(['Pais'])['salarioBase'].mean().sort_values(ascending = False)

df_salario_teste.drop(['location'], axis=1, inplace=True)

df_salario_teste['compania'] = df_salario_teste['compania'].str.upper()

df_salario_teste.to_csv('Base_normalizada_Leves_Fyi_Salary.csv', index= False)

"""# Etapa de Manipulação e Plotagem

Após normalizada Base, foi criado um novo dataFrame somente com o objetivo de manipulação e plotagem dos dados
"""

import pandas as pd

df_novo = pd.read_csv('/content/Base_normalizada_Leves_Fyi_Salary.csv')
df_novo.head()

"""Nova Matrix de Correlação Feita, para verificarmos quais colunas e tabela se correlacionam da melhor forma."""

df_novo.corr()

"""Definido em uma variavel as calunas tempo na empresa e salarioBase para verificarmos qual o compartamento, se quanto mais tempo na empresa maior seriam os salarios, para tal verificação utilizamos a media """

dg = df_novo.groupby("temponaEmpresa")['salarioBase'].mean()

dg.plot()

"""Podemos verificar que profissionais com ate 10 anos de empresa, acabam tendo muita variação de salario principalmente ao ingressar na empresa, apos 10 anos temos uma "normalização" e um pico entre os 20 a 30 anos.

Isso se da muito provavelmente pela area em questão pois a area de tecnologia é uma area muito volatil onde trabalhadores acabam traocando de empresa de uma forma muito rapida

Filtro feito para pegarmos somente as linhas que foram realmente informadas pelo usario na coluna raca
"""

dfiltrado_race = df_novo.loc[df_novo['raca'] != 'Não informado']
dfiltrado_race.head()

#Os dados de basesalary por raça tem 66% de dados faltantes
df_novo.groupby(["raca"])['salarioBase'].mean()

"""Plotagem feita com todos os dados inclusive os dados NA, para verificarmos o comportamento da amostra"""

df_novo.groupby(["raca"])['salarioBase'].mean().sort_values(ascending = False).plot(kind = "barh")

dfiltrado_race.groupby(["raca"])['salarioBase'].mean()

"""Plotagem feita com todos os dados Filtrados, dessa forma temos uma ideia mais real das racas com os salariosBase.

 Porem os asiaticos acabam sendo um total de 11 mil dos respondentes, isso provavelmente acaba jogando a media para baixo
"""

dfiltrado_race.groupby(["raca"])['salarioBase'].mean().plot()

"""**Verificação das colunas Pais e média do salario Base**

Podemos verificar que a uma variação muito grande entre os paises em questão
"""

df_novo.groupby(["Pais"])['salarioBase'].mean().plot()

"""Os top 10 paises com maiores salarios """

top_10_Pais = df_novo.groupby(["Pais"])['salarioBase'].mean().sort_values(ascending = False).head(10)

"""Plotagem com o Kindh para verificarmos visualmente os 10 maiores paises em questão de salario"""

top_10_Pais.sort_values(ascending = True).plot(kind = "barh")

"""Tabela para verificarmos os salarios bases de todos os paises envolvidos nas pesquisa em questão"""

df_novo.groupby(["Pais"])['salarioBase'].mean().sort_values(ascending = False)

"""Verificação da relação entre anos de experiencia e salario base """

df_novo.groupby(["anosExperiencia"])['salarioBase'].mean().sort_values(ascending = False).head(10)

"""Foi verificado que existe um pico muito ascentudo entre 0 e 10 anos se estabilizando logo a seguir, podemos chegar a conclusão que por conta da alta do profissional de tecnologia no mercado e com a possibilidade da home office alguns salarios subiram de forma ascentuada, ou são profissionais que são mais direcionados para linguagens mais utilizados no mercado"""

df_novo.groupby(["anosExperiencia"])['salarioBase'].mean().plot()

"""Verificacao de Salario Base em questão do genero, podemos verificar que dado a base em questão não temos uma diferença de salarios em questão de genero"""

df_novo.groupby(["genero"])['salarioBase'].mean()

df_genero_plot = df_novo.groupby(["genero"])['salarioBase'].mean()

df_genero_plot.plot(kind = "barh")

"""Verificação de salario Base pela educação, devido a area de tecnologia nãos ser uma area tão rigida como outras, há a possibilidade de haver desenvolvedores sem graduação mais elevadas trabalhando na area, dessa forma verificamos se existe realmente uma variação de salario devido a educação conquistada."""

df_novo.groupby(["Educacao"])['salarioBase'].count().sort_values(ascending = False)

df_novo.groupby(["Educacao"])['salarioBase'].mean().sort_values(ascending = False)

dfiltrado_educacao = df_novo.loc[df_novo['Educacao'] != 'Não informado']

df_educacao_plot = dfiltrado_educacao.groupby(["Educacao"])['salarioBase'].mean().sort_values(ascending = False)

df_educacao_plot.plot(kind = "bar")

"""Verificação de colunas Tempo na Empresa e ganhoAnual, verificamos se quanto mais tempo na empresa mais temos ganhos devido a experiencia"""

df_novo.groupby(["temponaEmpresa"])['ganhoAnual'].mean().sort_values(ascending = False).head(10)

df_novo.groupby(["temponaEmpresa"])['ganhoAnual'].mean().plot()

"""Verificação de colunas Compania e ganhoAnual, verificamos se quais empresas tem os maiores salarios anuais medios."""

df_filtrado_compania = df_novo.groupby(["compania"])['ganhoAnual'].mean().sort_values(ascending = False).head(10)

df_filtrado_compania.plot(kind = "barh")

"""Verificando as colunas genero por ganho atual, podemos ver claramente que as pessoas do genero feminino tem um menor ganho atual, sendo mais ou menos 5% comparado aos homens e comparado a outros é uma diferença de mais de 10%"""

df_novo.groupby(["genero"])['ganhoAnual'].mean().sort_values(ascending = False)

df_novo.groupby(["genero"])['ganhoAnual'].mean().plot()

"""Apesar de os Asiaticos estarem em ultimo lugar na verificação por salario base, na questão de salario Anual as cosias mudam onde as pessoas da colocaração negra acabam ficando em ultimo lugar"""

df_novo.groupby(["raca"])['ganhoAnual'].mean().sort_values(ascending = False)

dfiltrado_race.groupby(["raca"])['ganhoAnual'].mean().plot()

"""Fizemos as verificação para chegarmos a uma conclusão quais eram os maiores graus de escolaridade se era homens, mulheres, outros."""

# verificar a educação por gênero
Tabela_Cruzada = pd.crosstab(df_novo.genero, df_novo.Educacao)


Tabela_Cruzada.rename(columns ={"Bachelor's Degree": "Bacharelado",
                          "Highschool": "Ensino médio",
                          "Master's Degree":"Mestrado",
                          "PhD":"Doutorado",
                          "Some College":"Outros",
                          },
                inplace=True )


Tabela_Cruzada.loc['Total']= Tabela_Cruzada.sum(numeric_only=True, axis=0,)

Tabela_Cruzada.loc[:,'Total'] = Tabela_Cruzada.sum(numeric_only=True, axis=1)

Tabela_Cruzada

df_novo['profissao'].value_counts()

dfiltrado_gender = df_novo.loc[df_novo['genero'] != 'Não informado']

"""Empresas que pagam mais valores de ações por funcionario"""

df_novo.groupby(["compania"])['SGV'].mean().sort_values(ascending = False).head(10).plot(kind = "barh")

"""Verificação de quais são as empresas com mais respondentes a pesquisa"""

dfiltrado_gender.groupby(["compania"])['genero'].count().sort_values(ascending = False).head(10)

"""Verificação das companias que pagam os melhores salarios base em media"""

df_compania = df_novo.groupby(["compania"])['salarioBase'].mean().sort_values(ascending = False).head(10)

df_compania.plot(kind = 'barh')

"""Verificação dos salarios base por profissão, verificamos que apesar de possuirmos 41 mil engenheiro de softwares o mesmo ficou dentro da media"""

df_novo["profissao"].value_counts()

df_novo.groupby(["profissao"])['salarioBase'].mean().sort_values(ascending = False)

df_novo.groupby(["profissao"])['salarioBase'].mean().plot(kind = "bar")

"""verificamos para sermos mais precisos os anos de Experencia por salario base, tanto abaixo de 5 anos quanto acima de 10 anos."""

df_anosExperiencia_5 = df_novo.where(df_novo['anosExperiencia'] <= 5.0).groupby(['anosExperiencia'])['salarioBase'].mean()

df_anosExperiencia_5.plot()

df_anosExperiencia_5.plot(kind = "bar")

df_novo.where(df_novo['anosExperiencia'] >= 10.0).groupby(['anosExperiencia'])['salarioBase'].mean().sort_values(ascending = False)

"""Por fim verificamos para sermos mais precisos os tempo na empresa com o  salario base, tanto abaixo de 1 anos quanto acima de 10 anos."""

df_empresa_tempo = df_novo.where(df_novo['temponaEmpresa'] >= 10.0).groupby(['temponaEmpresa'])['salarioBase'].mean()

df_novo.where(df_novo['temponaEmpresa'] >= 10.0).groupby(['temponaEmpresa'])['salarioBase'].mean()

df_empresa_tempo.plot(kind = "barh")

df_novo.where(df_novo['temponaEmpresa'] <= 1.0).groupby(['temponaEmpresa'])['salarioBase'].mean()

df_empresa_1 = df_novo.where(df_novo['temponaEmpresa'] <= 1.0).groupby(['temponaEmpresa'])['salarioBase'].mean()

"""Verificamos que há uma queda brusca nos dados que remetem a 0.75 anos de empresa, provavelmente devido a variaçãod e dados referente a localização, pode haver um ruido em questão de paises com crises economicas, onde preferese diminuir o salario a demissão."""

df_empresa_1.plot(kind = 'bar')